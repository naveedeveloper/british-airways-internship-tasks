{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db57061",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b2422",
   "metadata": {},
   "source": [
    "## Web Scraping and Analysis\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e160e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05791dfe",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3987f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b466ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['reviews'] = reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60ccc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  Quick bag drop at First Win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  4 Hours before takeoff we r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  I recently had a delay on B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified |  Boarded on time, but it took a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  5 days before the flight, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |  Quick bag drop at First Win...\n",
       "1  ✅ Trip Verified |  4 Hours before takeoff we r...\n",
       "2  ✅ Trip Verified |  I recently had a delay on B...\n",
       "3  Not Verified |  Boarded on time, but it took a...\n",
       "4  ✅ Trip Verified |  5 days before the flight, w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195be23c",
   "metadata": {},
   "source": [
    "Now, We have the dataset for this task. The next thing that should be cleaned from this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36bf99",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b56f4",
   "metadata": {},
   "source": [
    "Removing the parts before | in the reviews column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b223b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(review):\n",
    "    return review.split(\"|\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f780a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviews = df.reviews.apply(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d537d89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quick bag drop at First Wing but too many pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 Hours before takeoff we received a Mail st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently had a delay on British Airways fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boarded on time, but it took ages to get to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 days before the flight, we were advised by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0    Quick bag drop at First Wing but too many pa...\n",
       "1    4 Hours before takeoff we received a Mail st...\n",
       "2    I recently had a delay on British Airways fr...\n",
       "3    Boarded on time, but it took ages to get to ...\n",
       "4    5 days before the flight, we were advised by..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e9ecd",
   "metadata": {},
   "source": [
    "### Rule-based approach\n",
    "\n",
    "This is a practical approach to analyzing text without training or using machine learning models. The result of this approach is a set of rules based on which the text is labeled as positive/negative/neutral. These rules are also known as lexicons. Hence, the Rule-based approach is called Lexicon based approach.\n",
    "\n",
    "Widely used lexicon-based approaches are TextBlob, VADER, SentiWordNet.\n",
    "\n",
    "### Data preprocessing steps:\n",
    "\n",
    "- Cleaning the text\n",
    "\n",
    "- Tokenization\n",
    "\n",
    "- Enrichment – POS tagging\n",
    "\n",
    "- Stopwords removal\n",
    "\n",
    "- Obtaining the stem words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad5cb9",
   "metadata": {},
   "source": [
    "#### Step 1: Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6aeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174f7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z]+', ' ', str(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669b68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned reviews'] = df.reviews.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f60bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>cleaned reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quick bag drop at First Wing but too many pa...</td>\n",
       "      <td>Quick bag drop at First Wing but too many pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 Hours before takeoff we received a Mail st...</td>\n",
       "      <td>Hours before takeoff we received a Mail stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently had a delay on British Airways fr...</td>\n",
       "      <td>I recently had a delay on British Airways fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boarded on time, but it took ages to get to ...</td>\n",
       "      <td>Boarded on time but it took ages to get to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 days before the flight, we were advised by...</td>\n",
       "      <td>days before the flight we were advised by BA ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    Quick bag drop at First Wing but too many pa...   \n",
       "1    4 Hours before takeoff we received a Mail st...   \n",
       "2    I recently had a delay on British Airways fr...   \n",
       "3    Boarded on time, but it took ages to get to ...   \n",
       "4    5 days before the flight, we were advised by...   \n",
       "\n",
       "                                     cleaned reviews  \n",
       "0   Quick bag drop at First Wing but too many pas...  \n",
       "1   Hours before takeoff we received a Mail stati...  \n",
       "2   I recently had a delay on British Airways fro...  \n",
       "3   Boarded on time but it took ages to get to th...  \n",
       "4   days before the flight we were advised by BA ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc8e93",
   "metadata": {},
   "source": [
    "#### Step 2: Tokenization\n",
    "Tokenization is the process of breaking the text into smaller pieces called Tokens. It can be performed at sentences(sentence tokenization) or word level(word tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc714d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['cleaned reviews'].apply(lambda txt: word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226c7c1",
   "metadata": {},
   "source": [
    "#### Step 3: Stopwords removal\n",
    "Stopwords in English are words that carry very little useful information. We need to remove them as part of text preprocessing. nltk has a list of stopwords of every language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748479d5",
   "metadata": {},
   "source": [
    "#### Step 4: Enrichment – POS tagging\n",
    "Parts of Speech (POS) tagging is a process of converting each token into a tuple having the form (word, tag). POS tagging essential to preserve the context of the word and is essential for Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63142504",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53426008",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1762a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos tagged'] = df['cleaned reviews'].apply(token_stop_pos)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f14911",
   "metadata": {},
   "source": [
    "#### Step 5: Obtaining the stem words\n",
    "A stem is a part of a word responsible for its lexical meaning. The two popular techniques of obtaining the root/stem words are Stemming and Lemmatization.\n",
    "\n",
    "The key difference is Stemming often gives some meaningless root words as it simply chops off some characters in the end. Lemmatization gives meaningful root words, however, it requires POS tags of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c91d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63912ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1214f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemma'] = df['pos tagged'].apply(lemmatize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbf873",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using VADER\n",
    "VADER stands for Valence Aware Dictionary and Sentiment Reasoner.\n",
    "\n",
    "Vader sentiment not only tells if the statement is positive or negative along with the intensity of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a632d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    return vs['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175464ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Lemma'].apply(vadersentimentanalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a303b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_analysis(compound):\n",
    "    if compound >= 0.5:\n",
    "        return 'Positive'\n",
    "    elif compound < 0 :\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2017cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Analysis'] = df['Sentiment'].apply(vader_analysis)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_counts = df['Analysis'].value_counts()\n",
    "vader_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f37e0",
   "metadata": {},
   "source": [
    "#### Visual Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b249de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c48fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Reviews Analysis\")\n",
    "plt.pie(vader_counts.values, labels = vader_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99445216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"British_airways_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f8864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
